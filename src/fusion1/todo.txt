General TODO:

* setup vision and hearing pipelines
* build age estimation -> OpenBiometrics problem
* build gender estimation -> OpenBiometrics problem
* id, age and gender estimation cannot run asynchronously, so they have to be embedded in vision pipeline somehow
* create directory structure and store all thumbs
* hook up realsense faces
* cut out realsense thumbs and send them over, depth is descoped for now
* adjustable timer periods and FOVs and more parameters
* refactor saliency for all cameras
* hook up rviz (analyze current perception ROS node)
* make transformations work from motor positions (refactor current perception ROS node)
* refactor towards using dynamic reconfigure, instead of separate messages
* disable/enable thumb storage
* disable/enable sound storage
* disable/enable debug windows
* disable/enable rviz
* make sure camera FOV works correctly
* make face height global parameter
+ fuse observations (cusers between pipelines, chands between pipelines, saliency points between pipelines, cusers+saliency points, hands+saliency points, sounds+cusers, sounds+hands, sounds+salient points, speech+cusers, speech+salient points)
+ output established users, hands, salient points and speech
- maintain global CSV file with parameters for stored faces (timestamp, anything already gathered from realsense, etc.)
- refactor room luminance for all cameras
- make wiki for the system

RealSense leftover TODO:

+ hook up RealSense hands
+ implement saliency for RealSense, or use blob detection that comes with RealSense
+ access ROS parameters in the RealSense code (with messages, dynamic reconfigure might still be too far off)
- solve timing problem between Linux and Windows (first send ROS message to RealSense, wait for answer, the time difference divided by 2 is the general delay between Windows and Linux), this might not be necessary

Visualization TODO:

+ visualize users in rviz
+ visualize saliency in rviz

Audio leftover TODO:

+ output sounds and speech to directories and CSV
+ hook up speech and audio localization from both microphones
- audio detection and streaming, refactor audio_stream and audio_sensor
- google speech, continuous and regular, refactor google_speech
- dragon speech, continuous and regular, append to RealSense



Towards OpenCog:

	for each face:
		position in 3D (robot coordinates)
		confidence of this position (0..1)
		how much she smiles (0..1) --> predicates (truthvalue)
		how much she frowns (0..1)
		various analyzed expressions (list of strings), TBD
		gender, if found (None, Male, Female)
		confidence in gender (0..1)
		age, if found (None, 0..200)
		confidence in age (0..1)
		identity (64-bit ID, or string, could be more closely related to known atoms)
		confidence in identity
		... and whatever Ethiopia and Ralf come up with that might be interesting

	for each hand:
		position in 3D (robot coordinates)
		confidence of this position (0..1)
		various analyzed gestures (list of strings), TBD

	for each salient point:
		position in 3D (or only an angle.. not sure yet)
		confidence (0..1; if two cameras see something interesting, it must be really interesting)

	for each microphone:
		translated speech
		confidence of the translation (0..1)
		localization of where the speech came from (angle, maybe quaternion)
		confidence of the location
