General TODO:

* setup vision and hearing pipelines
* build age estimation -> OpenBiometrics problem
* build gender estimation -> OpenBiometrics problem
* id, age and gender estimation cannot run asynchronously, so they have to be embedded in vision pipeline somehow
* create directory structure and store all thumbs
* hook up realsense faces
* cut out realsense thumbs and send them over, depth is descoped for now
* adjustable timer periods and FOVs and more parameters
* refactor saliency for all cameras
* hook up rviz (analyze current perception ROS node)
* make transformations work from motor positions (refactor current perception ROS node)
* refactor towards using dynamic reconfigure, instead of separate messages
* disable/enable thumb storage
* disable/enable sound storage
* disable/enable debug windows
* disable/enable rviz
* make sure camera FOV works correctly
* make face height global parameter
* hook up RealSense hands
* access ROS parameters in the RealSense code (with messages, dynamic reconfigure might still be too far off)
+ make all nodes properly listen to dynamic reconfigure
+ documentation
+ fuse observations (cusers between pipelines, chands between pipelines, saliency points between pipelines, cusers+saliency points, hands+saliency points, sounds+cusers, sounds+hands, sounds+salient points, speech+cusers, speech+salient points)
+ output established users, hands, salient points and speech
- maintain global CSV file with parameters for stored faces (timestamp, anything already gathered from realsense, etc.)
- refactor room luminance for all cameras (maybe add to saliency code)
- detect_faces: Haar cascade -> ROS parameter
- detect_saliency: also output the actual feature it triggered on


Visualization TODO:

+ visualize users in rviz
+ visualize saliency in rviz


Back home:

+ output sounds and speech to directories and CSV
+ hook up speech and audio localization from both microphones
- audio detection and streaming, refactor audio_stream and audio_sensor
- google speech, continuous and regular, refactor google_speech
- dragon speech, continuous and regular, append to RealSense
+ implement saliency for RealSense (needs video rescale and faster way to do the OpenCV algorithms)


Descoped:

- use dynamic reconfigure parameters for all RealSense configuration stuff (i.e. reduce status window on Windows side, join with removing the old topics and features later on)
- solve timing problem between Linux and Windows (first send ROS message to RealSense, wait for answer, the time difference divided by 2 is the general delay between Windows and Linux), this might not be necessary


Towards OpenCog:

	for each face:
		position in 3D (robot coordinates)
		confidence of this position (0..1)
		how much she smiles (0..1) --> predicates (truthvalue)
		how much she frowns (0..1)
		various analyzed expressions (list of strings), TBD
		gender, if found (None, Male, Female)
		confidence in gender (0..1)
		age, if found (None, 0..200)
		confidence in age (0..1)
		identity (64-bit ID, or string, could be more closely related to known atoms)
		confidence in identity
		... and whatever Ethiopia and Ralf come up with that might be interesting

	for each hand:
		position in 3D (robot coordinates)
		confidence of this position (0..1)
		various analyzed gestures (list of strings), TBD

	for each salient point:
		position in 3D (or only an angle.. not sure yet)
		confidence (0..1; if two cameras see something interesting, it must be really interesting)

	for each microphone:
		translated speech
		confidence of the translation (0..1)
		localization of where the speech came from (angle, maybe quaternion)
		confidence of the location


Face: 28558a3a6ce84fb5228c6afc3566f32b
Hand: 6253b9f7372c469c55267f148fc989c3
Saliency: d8d3b123b8387b96db653a2e592d4e65
RealSenseParam: 391bf7fa8780bd9d4a2254dc9194b8b0
